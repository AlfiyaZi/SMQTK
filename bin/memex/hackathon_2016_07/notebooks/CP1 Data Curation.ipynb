{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_FILE = '../data/official/CP1_train_ads.json' # 8226390d0e8ce5f95dd72f54efa43aa4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8226390d0e8ce5f95dd72f54efa43aa4  ../data/official/CP1_train_ads.json\r\n"
     ]
    }
   ],
   "source": [
    "!md5sum $DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curating Positive Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdr_ids = set()\n",
    "\n",
    "with open(DATA_FILE, 'rb') as infile:\n",
    "    for line in infile:\n",
    "        ad = json.loads(line.strip())\n",
    "        cdr_ids.add(ad['doc_id'])\n",
    "        \n",
    "with open('../data/CP1_cdr_ids.txt', 'wb') as outfile:\n",
    "    for cdr_id in cdr_ids:\n",
    "        outfile.write('%s\\n' % cdr_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "real\t10m1.335s\r\n",
      "user\t5m15.614s\r\n",
      "sys\t1m3.469s\r\n"
     ]
    }
   ],
   "source": [
    "!time parallel -j4 --joblog ../data/misc/get_es_child_documents.log \\\n",
    "                   --arg-file ../data/CP1_cdr_ids.txt \\\n",
    "                   --retries 3 \\\n",
    "                   --max-args 100 \\\n",
    "                   python ../scripts/get_es_child_documents.py >> ../data/CP1_image_documents.json\n",
    "                \n",
    "# View any jobs that failed:\n",
    "# awk '$7 != 0' ../data/misc/get_es_child_documents.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sort -u ../data/CP1_image_documents.json > tmp.json\n",
    "!mv tmp.json ../data/CP1_image_documents.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figure out unique URLs in the image documents and map them to their doc_ids for a sane downloaded name\n",
    "image_url_to_doc_ids = defaultdict(set)\n",
    "\n",
    "with open('../data/CP1_image_documents.json', 'rb') as infile:\n",
    "    for line in infile:\n",
    "        image_doc = json.loads(line)\n",
    "        \n",
    "        if image_doc['obj_stored_url']:\n",
    "            image_url_to_doc_ids[image_doc['obj_stored_url']].add(image_doc['doc_id'])\n",
    "        \n",
    "# Construct file for parallel downloading\n",
    "with open('../data/CP1_image_urls.txt', 'wb') as outfile:\n",
    "    for (url, doc_ids) in image_url_to_doc_ids.iteritems():\n",
    "        outfile.write(' '.join([url] + list(doc_ids)) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "# Retries are important here since servers can get bogged down\n",
    "# The joblog will reveal with the exit status of the script to determine\n",
    "# if anything may be able to be retrieved\n",
    "!time parallel -j10 \\\n",
    "               --joblog ../data/misc/dl_images.log \\\n",
    "               --arg-file ../data/CP1_image_urls.txt \\\n",
    "               --retries 3 \\\n",
    "               --colsep ' ' \\\n",
    "               python ../scripts/download_url_for_doc_ids.py > ../data/CP1_url_sha.txt\n",
    "            \n",
    "!find ../data/CP1_imageset -type f -size 0 -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sort -u ../data/CP1_url_sha.txt > tmp.txt\n",
    "!mv tmp.txt ../data/CP1_url_sha.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalize CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Curate a standard CSV\n",
    "# cluster_id,ad_id,sha1\n",
    "\n",
    "# CP1_image_documents.json gives ad_id and image doc_id which can be mapped to sha via image_doc_id_sha\n",
    "clusters_ads = set()\n",
    "\n",
    "# cluster_id,ad_id are given from the official DATA_FILE\n",
    "with open(DATA_FILE, 'rb') as infile:\n",
    "    for line in infile:\n",
    "        ad = json.loads(line.strip())\n",
    "        clusters_ads.add((ad['cluster_id'], ad['doc_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203172 ads across 450 clusters.\n"
     ]
    }
   ],
   "source": [
    "print '%d ads across %d clusters.' % (len(set([x[1] for x in clusters_ads])),\n",
    "                                      len(set([x[0] for x in clusters_ads]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the sha1 through the CP1_url_shas.txt through the obj_stored_url in CP1_image_documents.json\n",
    "# Going from image to ad will always yield an ad\n",
    "with open('../data/CP1_image_documents.json', 'rb') as infile:\n",
    "    ad_id_image_urls = defaultdict(set)\n",
    "    \n",
    "    for line in infile:\n",
    "        image_doc = json.loads(line)\n",
    "        \n",
    "        if not isinstance(image_doc['obj_parent'], list):\n",
    "            image_doc['obj_parent'] = [image_doc['obj_parent']]\n",
    "                \n",
    "        for ad_id in image_doc['obj_parent']:\n",
    "            ad_id_image_urls[ad_id].add(image_doc['obj_stored_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914576 image URLs exist across 121901 ads.\n"
     ]
    }
   ],
   "source": [
    "print '%d image URLs exist across %d ads.' % (sum([len(x) for x in ad_id_image_urls.values()]), len(ad_id_image_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Go from obj_stored_url to sha1\n",
    "with open('../data/CP1_url_sha.txt', 'rb') as infile:\n",
    "    url_sha = {}\n",
    "    \n",
    "    for line in infile:\n",
    "        url, sha = line.strip().split()\n",
    "        url_sha[url] = sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232794 unique URLs.\n"
     ]
    }
   ],
   "source": [
    "print '%d unique URLs.' % len(url_sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../data/CP1_clusters_ads_images.csv', 'wb') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Headers\n",
    "    writer.writerow(['cluster_id', 'ad_id', 'sha1'])\n",
    "    \n",
    "    for (cluster_id, ad_id) in clusters_ads:\n",
    "        # Finding a url should always work\n",
    "        # Finding the sha from a url may not, if the URL failed to be retrieved (404, whatever)\n",
    "        image_urls_from_ad = ad_id_image_urls[ad_id] \n",
    "        image_shas_from_ad = set([url_sha[x] for x in image_urls_from_ad if x in url_sha])\n",
    "        \n",
    "        for sha1 in image_shas_from_ad:\n",
    "            writer.writerow((cluster_id, ad_id, sha1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curating Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
